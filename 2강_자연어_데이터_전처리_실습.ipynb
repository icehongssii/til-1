{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2강 자연어 데이터 전처리 실습.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOelQhMgNjbHDaeeURmrFhl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/icehongssii/til-1/blob/master/2%EA%B0%95_%EC%9E%90%EC%97%B0%EC%96%B4_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC_%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlWzv44i2ocU"
      },
      "source": [
        "# 2강 자연어 데이터 전처리\n",
        "\n",
        "[[SB]+딥러닝+자연어+처리_2주차_원본.pdf](2%E1%84%80%E1%85%A1%E1%86%BC%20%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%207678562c98b54a80902190e8d0a8ae90/SB_2_.pdf)\n",
        "\n",
        "# 학습목표\n",
        "\n",
        "1. 자연어 데이터 묶음 코퍼스 설명가능\n",
        "2. NLTL를 활용해 코퍼스 분석 가능\n",
        "3. 텍스트 수준 텍스트 전처리 하는 방법 설명가능\n",
        "4. 단어 수준 텍스트 전처리 하는 방법 설명가능\n",
        "5. 정규 표현식 이해 가능 \n",
        "\n",
        "# 대단원\n",
        "\n",
        "1. 자연어 데이터\n",
        "2. 텍스트 데이터 전처리\n",
        "\n",
        "# 질문\n",
        "\n",
        "1. 코퍼스로 우리는 무엇을 할 수 있는가?\n",
        "2. nltk란 무엇인가?\n",
        "3. 데이터 토큰화는 무엇인가?\n",
        "4. nltk에서 어간을 추출하는 방법은 어떤것이있는가?\n",
        "5. 불용어는 무엇인가?\n",
        "\n",
        "# 자연어데이터\n",
        "\n",
        "## 1. 코퍼스\n",
        "\n",
        "- 데이터를 볼 때 음성데이터, 텍스트, 수어라는 데이터로 나누어지고 교육 과정에서는 텍스트에 대해 볼 것\n",
        "- 텍트스 데이터를 코퍼스(말뭉치)라고 한다\n",
        "- 코퍼스는 NLP연구를 위해특정한 목적을 가지고 언어를 표출한 집합이다\n",
        "- 코퍼스가 둘 이상이면 코포라 라고 읽는다\n",
        "\n",
        "예) 감정분석 프로그램을 위해 감정이 담긴 데이터를 준비한다. 이 데이터를 뭉쳐 놓으면 감정 분석에 대한 코퍼스라고 한다. \n",
        "\n",
        "예)셰익스피어 문체 소설 작성시 세익스피어의 데이터 집합을 코퍼스라고 한다\n",
        "\n",
        "## 2. 코퍼스가 필요한 이유\n",
        "\n",
        "- 코퍼스의 도움으로 자연어에 대한 빈도 분포, 단어의 동시 발생등과 같은 통계적 분석 가능하다(특정단어가 많이 나오면 부정적, 긍정적, 통계적인 의미를 가질 수 있는데 코퍼스로 가능하다) 어떤 단어가 너무 빈번하게 나온다면 그 단어의 의미는 없다등으로 예측할 수 있다.\n",
        "- 다양한 자연어 처리 과정에서 자연어 데이터에 대한 언어 규칙을 정의하고 이를 검정 할 수 있다 (문법체계가 있는 글이나 사투리에 대한 글이나 문법적인 것을 분석할 수 있는 토대가 될 수 있다)\n",
        "- 규칙기반 시스템의 도움으로 언어 사용에 따라 각 언어에 대한 언어 규칙을 정의할 수 있다 (언어에 대한 룰을 잘 안다면 각 언어에 대한 규칙을 정의해 응용이 가능하다)\n",
        "\n",
        "## 3. 코퍼스 수집\n",
        "\n",
        "- 단일언어 코퍼스 (한가지 언어)\n",
        "- 이중언어 코퍼스 (두가지 언어)\n",
        "- 다국어 코퍼스 (여러가지 언어)\n",
        "- 수집방법\n",
        "\n",
        "    공개된 오픈소스 코퍼스 사용(NLTK, koNLTK)\n",
        "\n",
        "    다양한 웹사이트에서 크롤링 통한 자연어 데이터 수집 (저작권 조심)\n",
        "\n",
        "## 4. 코퍼스 분석\n",
        "\n",
        "1. 음성데이터에서의 코퍼스 분석 : 각 데이터 음성 이해에 대한 분석/대화분석\n",
        "2. 텍스트 데이터에서의 코퍼스 분석\n",
        "- 일반적으로 코퍼스에 단어가 몇개 나오는지 특정단어 빈도수가 얼마인지 분석\n",
        "- 코퍼스에 노이즈가 있으면 노이즈 제거\n",
        "\n",
        "## 5. NLTK(Natural Language Toolkit)\n",
        "\n",
        "- 영어 자연어 처리를 위한 라이브러리 및 프로그램 모음\n",
        "- 다양한 기능을 가지고 있고 교육용 뿐만 아니라 실무 및 연구에서도 많이 사용\n",
        "- 50종류 이상 코포라와 어휘자원 갖고 있는 오픈소스\n",
        "\n",
        "![2%E1%84%80%E1%85%A1%E1%86%BC%20%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%207678562c98b54a80902190e8d0a8ae90/Untitled.png](2%E1%84%80%E1%85%A1%E1%86%BC%20%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%207678562c98b54a80902190e8d0a8ae90/Untitled.png)\n",
        "\n",
        "# 데이터 전처리\n",
        "\n",
        "## 1. 데이터 전처리\n",
        "\n",
        "어떤 작업 진행전 주어진 데이터 목적에 맞추어 변형 또는 가공하는 과정. 이미 전처리 데이터가 아닌 직접 크롤링하여 수집한 데이터는 결과 정확성을 위해 전처리는 필수적이다.\n",
        "\n",
        "예)AI는 데이터에 의존성이 높다. 인공지능이 알아들을 수 있도록 데이터의 노이즈를 제거하여 학습시키는게 중요\n",
        "\n",
        "- **자연어처리에서 데이터 처리과정**\n",
        "\n",
        "    토큰화 → 정제 → 정규화\n",
        "\n",
        "    토큰화 : 문법에서 가장 작은 단위\n",
        "\n",
        "    정제 : 의미가 없는 노이즈 데이터를 지운다\n",
        "\n",
        "    정규화(normalization) : 어순이나 값의 크기도 모두 다르다 NMT번역 시 같은 형태로 맞춰주는 둥 정규화 과정이 필요하다\n",
        "\n",
        "## 2. 데이터 토큰화\n",
        "\n",
        "주어진 코퍼스를 토큰이라는 단위로 나누는 과정\n",
        "\n",
        "- 문장, 단어, 구두점 등 다양한 단위로 토큰 설정가능하다\n",
        "- nltk, OpenNLP, kss등 토큰화 수행할 수 있는 다양한 오픈소스 툴킷이 존재한다. (토크나이징의 방법은 다양하다)\n",
        "\n",
        "텍스트 데이터에서 데이터는 단락 형태이므로 단락에서 문장을 문장레벨로 토큰화 하는 과정이 필요.\n",
        "\n",
        "글>문단>문장>단어 토크나이징이라는 기본적으로 문단을 문장으로, 문장을 문장을 단어로 쪼개고 점점 작은 단위로 쪼개는 것을 의미한다.\n",
        "\n",
        "코퍼스 만들 때 보통 문단이므로 문장단위로 토큰화 할 필요가 있다.\n",
        "\n",
        "단어토큰화는 텍스트들을 단어, 구, ,의미있는 문자열로 자르는 과정이다.\n",
        "\n",
        "토크나이징의 방법은 여러가지이므로 내가 만들 어플리케이션의 종류에 따라 필요한 데이터/코퍼스를 정의하고 그 코퍼스에 맞는 function을 찾아 검색해서 들어가는 것이 중요하다.\n",
        "\n",
        "1. **상황과 목적에 맞는 토큰화 툴을 사용한다.** \n",
        "\n",
        " nltk의  word_tokenize는 의미에 따라 단어 분리하지만WordPunctTokenizer의 경우 구두점을 기준으로 단어를 분리하므로 알맞은 토큰화툴사용할것\n",
        "\n",
        "don't → do, n't (word_tokenize)\n",
        "\n",
        "don't→ don, ', t (WordPunctTokenizer)\n",
        "\n",
        "2. **구두점이나 특수문자 포함시**\n",
        "\n",
        "Ph.d, $7.49등 온점이 포함되있는 경우  토큰화 조심할 것\n",
        "\n",
        "## 3. 데이터 정제와 데이터 정규화\n",
        "\n",
        "1. **데이터 정제(data cleaning/cleansing)**\n",
        "\n",
        "데이터에서 손상되거나 부정확한 부분 감지하고 수정하는 과정 또는  데이터의 불완전하거나 부정확한 부분을 식별 후 해당 부분 대체,수정,삭제하는과정이라고 한다.\n",
        "\n",
        "- 유효한 데이터 :  시그널\n",
        "- 방해하는 데이터 : 노이즈\n",
        "\n",
        "노이즈를 줄여 시그널을 높이는 데이터를 찾는다\n",
        "\n",
        "**2. 데이터 정규화(data normalization)**\n",
        "\n",
        "표현방법이 다른 데이터를 같은 단어로 통합시키는 과정이다 \n",
        "\n",
        "## 4. 대소문자 통합\n",
        "\n",
        "영어권에서 대소문자 통합과정은 보통 소문자로 변환한다\n",
        "\n",
        "## 5. 어간추출(stemming)\n",
        "\n",
        "어간 : 가장 중심되는 단어\n",
        "\n",
        "접미사를 삭제하거나 대체해 문장의 각 단어를 어근형태로 변환하는 과정 \n",
        "\n",
        "접미사를 삭제또는 대체하는 과정이므로 결과물이 사전에 존재하지 않을 수도 있다\n",
        "\n",
        "다양한 어간 추출 툴킷이 존재하지만 언어별로 잘 동작하지 않을 수 있음\n",
        "\n",
        "## 6. 표제어 추출(lemmatizing)\n",
        "\n",
        "단어에 있는 품사를 파악해 정확하게 의도된 품사와 문장에 있는 단어를 식별하는과정(POS, part-of-speech)\n",
        "\n",
        "활용어미 없애고 단어를 사전이나 어휘에있는 기본형으로 변환\n",
        "\n",
        "Worndet과 같은 태그가 있는 사전을 사용해 가공정 텍스트를 표제어로 변환\n",
        "\n",
        "- 품사태그\n",
        "\n",
        "    단어는 형태가 같아도 품사에 따라 의미가 크게 달라질 수가 있다정의와 문맥에 따라 텍스트의 특정 부분에 해당하는 단어를 마크업하는 과정 \n",
        "\n",
        "- 불용어\n",
        "\n",
        "    무시할 수 있거나 최소한적 의미를 가치를 지닌 단어"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8CHhJ6paidT"
      },
      "source": [
        "NLTK 사용\n",
        "### 1. nltk 다운로드 및 설치\n",
        "### 2. nltk corpus 사용법\n",
        "### 3. nltk tokenizing\n",
        "### 4. nltk stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGG4ejPaarWs"
      },
      "source": [
        "# nltk install\n",
        "!pip install --user -U nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLo0z2i8cpfp"
      },
      "source": [
        "import nltk\n",
        "import textwrap"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fdQ-4pldD_h"
      },
      "source": [
        "## corpus \n",
        "1. isolate corpus : gutenberg, webtext\n",
        "2. categorized corpus : brown\n",
        "3. overlapping corpus\n",
        "4. temporal corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hx6RSPGdJDM"
      },
      "source": [
        "from nltk.corpus import brown as cb\n",
        "from nltk.corpus import gutenberg as gb"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i4PMB7odQKA"
      },
      "source": [
        "nltk.download('brown')\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBbLBMUgdZ4w",
        "outputId": "fa639118-3250-4467-9f46-c9637c83bd91"
      },
      "source": [
        "#여러가지 파일 형태로 되어있다는 확인\n",
        "cb.fileids()[:4]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ca01', 'ca02', 'ca03', 'ca04']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1aRDlZGderf",
        "outputId": "9c5a6d64-668b-4d70-aa69-8d894e98f5f0"
      },
      "source": [
        "gb.fileids()[:5]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEmfWaB9dj1r",
        "outputId": "02295349-3314-422e-c05d-e105d3b25f41"
      },
      "source": [
        "#brown은 카테고리 코퍼스 이므로 아래와 같이 카테고리가 나눠짐\n",
        "cb.categories()[:3]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adventure', 'belles_lettres', 'editorial']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6acbTTczdt0y",
        "outputId": "68f695cd-d8f7-4b5d-c8d8-4e274e82aeb8"
      },
      "source": [
        "print(len(cb.words())) #브라운 코퍼스 총 단어 갯수 \n",
        "print(len(gb.words())) #구텐버그 코퍼스 총 단어 갯수"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1161192\n",
            "2621613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4I1u-vrd7Yh",
        "outputId": "3cd8d9eb-bdac-407b-e7b5-2bfd9e7ea1ed"
      },
      "source": [
        "#brwon corpus categorical 단어 갯수 \n",
        "print(len(cb.words(categories='adventure')))\n",
        "\n",
        "#특정 fileids에 있는 단어 출력\n",
        "print(cb.words(fileids='ca01'))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69342\n",
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nquyl0f4eNJR",
        "outputId": "886e2605-a6c1-4e9d-80c7-7d1c634b2106"
      },
      "source": [
        "# gutenberg corpus file 별로 단어를 확인하기\n",
        "gb.words(fileids='austen-emma.txt')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z6LPLudeosH",
        "outputId": "dfa713a7-40bd-4fb8-efa0-07b41d64ce42"
      },
      "source": [
        "#텍스트 전체를 가져오기 \n",
        "raw_text = cb.raw(categories='adventure')\n",
        "#brown 파일아이디로 전체 가져오기. 대신 70바이트로 잘랐을 때 첫번째 인덱스\n",
        "print(textwrap.wrap(cb.raw(fileids='ca01'),width=70)[0])\n",
        "\n",
        "#brown 카테고리 별로 가져오기 대신 70바이트로 잘랐을 때 첫번째 인덱스\n",
        "print(textwrap.wrap(cb.raw(categories='adventure'),width=70)[0])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          The/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl\n",
            "          Dan/np Morgan/np told/vbd himself/ppl he/pps would/md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMQd587yembF",
        "outputId": "942c7495-8e7b-4b03-aa86-800a2ab5f75e"
      },
      "source": [
        "#gutenberg 파일아이디로 전체 가져오기. 대신 70바이트로 잘랐을 때 첫번째 인덱스\n",
        "print(textwrap.wrap(gb.raw(fileids='austen-emma.txt'),width=70)[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Emma by Jane Austen 1816]  VOLUME I  CHAPTER I   Emma Woodhouse,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmHRD03XgWVN"
      },
      "source": [
        "## NLTK Tokenizing\n",
        "\n",
        "글>문단>문장>단어\n",
        "토크나이징이라는 기본적으로 문단을 문장으로, 문장을 문장을 단어로 쪼개고 점점 작은 단위로 쪼개는 것을 의미한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e5xCLOwgul_"
      },
      "source": [
        "# tokenizing\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "raw_text = gb.raw(fileids='austen-emma.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beUsPNywgyKj",
        "outputId": "b3c942a3-4ff9-47ac-eb88-39fa5e48e3e0"
      },
      "source": [
        "# senetence tokenize 문단 -> 문장 \n",
        "sents = sent_tokenize(raw_text)\n",
        "print(len(sents))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmwVmv8Wg5zz"
      },
      "source": [
        "# workd tokenizing 문장 -> 단어\n",
        "from nltk.tokenize import word_tokenize "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDoWR4Geh6Yc",
        "outputId": "b345eec0-d2f6-407d-eb1f-abfa4b5730e6"
      },
      "source": [
        "word_tokenize(sents[0])[:10]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq1ticqUiBht",
        "outputId": "1c74f72b-45f4-49e7-966a-7e0eddfd3311"
      },
      "source": [
        "# word punctTokenizer\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "wpt = WordPunctTokenizer()\n",
        "wpt.tokenize(sents[0])[:10]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR2vL2NfiU0u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgw-1vEAiYnk"
      },
      "source": [
        "## NLTK Stemming 어간 추출\n",
        "어간은 활용어가 사용할 때 변하지 않는 부분을 어간이라고한다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km0luqaMiha3"
      },
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1mWYq29ikts",
        "outputId": "afb48345-919d-48b6-eaba-4dad30990257"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "tokens = word_tokenize(sents[0])\n",
        "#어간추출\n",
        "list(map(ps.stem,tokens))[:15]"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'emma',\n",
              " 'by',\n",
              " 'jane',\n",
              " 'austen',\n",
              " '1816',\n",
              " ']',\n",
              " 'volum',\n",
              " 'I',\n",
              " 'chapter',\n",
              " 'I',\n",
              " 'emma',\n",
              " 'woodhous',\n",
              " ',',\n",
              " 'handsom']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCm5dOIliw91",
        "outputId": "5becb50a-ee4a-471e-8077-7ce5cede6790"
      },
      "source": [
        "#어간 추출전 토큰\n",
        "tokens[:15]"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'Emma',\n",
              " 'by',\n",
              " 'Jane',\n",
              " 'Austen',\n",
              " '1816',\n",
              " ']',\n",
              " 'VOLUME',\n",
              " 'I',\n",
              " 'CHAPTER',\n",
              " 'I',\n",
              " 'Emma',\n",
              " 'Woodhouse',\n",
              " ',',\n",
              " 'handsome']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    }
  ]
}